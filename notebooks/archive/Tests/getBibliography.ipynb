{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Bibliography\n",
    "This notebooks collects all bibliographic information of a given book from LNCS (URL).\n",
    "###### written by Nils Dyck, 29.09.2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloads the html-Code of the given URL and cooks the soup\n",
    "\n",
    "import urllib.request, urllib.error, urllib.parse, re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = input('Enter URL to download: ')\n",
    "\n",
    "response = urllib.request.urlopen(url)\n",
    "webContent = response.read().decode('UTF-8')\n",
    "\n",
    "soup = BeautifulSoup(webContent, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Markus Holzer', 'Christian Rauch'], ['Hermann Gruber', 'Christian Rauch'], ['Hermann Gruber', 'Markus Holzer']]\n",
      "['Markus Holzer, Christian Rauch', 'Hermann Gruber, Christian Rauch', 'Hermann Gruber, Markus Holzer']\n"
     ]
    }
   ],
   "source": [
    "#finding authors and saving them in a list called 'author_list'\n",
    "authors = soup.find_all(href = re.compile('#auth.*'))\n",
    "author_list = []\n",
    "for author in authors:\n",
    "    for string in author.stripped_strings:\n",
    "        #print(repr(string))\n",
    "        author_list.append(string)\n",
    "author_list\n",
    "co_authors = []\n",
    "for i in range(len(author_list)):\n",
    "    co_authors.append([])\n",
    "    for j in range(len(author_list)):\n",
    "        if i != j:\n",
    "            co_authors[i].append(author_list[j])\n",
    "print(co_authors)\n",
    "co_author_string = []\n",
    "for k in range(len(co_authors)):\n",
    "    co_author_string.append('')\n",
    "    for l in range(len(co_authors[k])):\n",
    "        co_author_string[k]+= (co_authors[k][l]+', ')\n",
    "    co_author_string[k] = co_author_string[k][:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Most regular-expression matching engines in practice are based on the Thompson construction and the Spencer matching algorithm. While these engines work fast and efficiently, a serious problem, the regular expression denial-of-service\\xa0(ReDoS), has been reported recently. ReDoS is an algorithm complexity attack, which exploits the backtracking feature of the engine, and makes the service unresponsive indefinitely. Researchers suggested a few remedies to cope with the ReDoS problem, yet they are often ad-hoc or undesirable in practice. We instead propose a hybrid matching scheme that selects between the Thompson and the Spencer matching algorithms depending on the needed features. We also suggest to use the position construction for its intrinsic characteristics for fast matching. We evaluate the proposed approach using a benchmark dataset collected from various open-source projects, and compare the performance with the current approach. The experimental results show that a hybrid matcher reduces the ReDoS-vulnerability by 96% and 99.98% in full and partial matching, respectively. Moreover, 55% of the most problematic regular expressions become invulnerable to ReDoS by the position construction.'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finding the abstract and saving it in a string calle 'abstract_string'\n",
    "abstract = soup.find(class_ = 'c-article-section__content')\n",
    "abstract_string = abstract.find('p')\n",
    "#Cleaning up the abstract a little bit\n",
    "abstract_string = str(abstract_string).replace(\";\",\".\") #for the csv-file there must not be any semicola in the abstracts\n",
    "abstract_string = re.sub(\"<.*?>\",\"\",abstract_string) #no html-tag should survive this action.\n",
    "abstract_string = abstract_string.replace(\"\\n\",\"\")\n",
    "abstract_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://doi.org/10.1007/978-3-031-07469-1_3\n",
      "28 May 2022\n",
      "Springer, Cham\n",
      "978-3-031-07468-4\n",
      "978-3-031-07469-1\n"
     ]
    }
   ],
   "source": [
    "#finding other bibliographic information\n",
    "bi = soup.find_all(class_ = 'c-bibliographic-information__value')\n",
    "for b in bi:\n",
    "    print(b.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<generator object PageElement.stripped_strings at 0x7fedc13de740>'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finding venue\n",
    "venue = soup.find(class_ = 'c-chapter-book-details right-arrow')\n",
    "venue = repr(venue.stripped_strings)\n",
    "venue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How to Settle the ReDoS Problem: Back to the Classical Automata Theory'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title = soup.title\n",
    "title = str(title)\n",
    "title = title.replace('\\xa0', ' ')\n",
    "title = re.sub(\"<.*?>\", \"\", title) #no html-tag should survive this action.\n",
    "title = title.replace(' | SpringerLink', '')\n",
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('collaboration.csv', 'w') as f:\n",
    "    for i in range(len(author_list)):\n",
    "        f.write(author_list[i]+';')\n",
    "        for j in range (len(author_list)):\n",
    "            if j != i:\n",
    "                f.write(author_list[j]+', ')\n",
    "        f.write(';')\n",
    "        f.write(title+';')\n",
    "        f.write(bi[0].string+', '+bi[1].string+', '+bi[2].string+', '+bi[3].string+', '+bi[4].string+';')\n",
    "        f.write(abstract_string)\n",
    "        f.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d486b63b850724bc6551cda0a2a2678785b3f8f51c1b5738c292f3857cafcf62"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
